{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from numpy.random import permutation as perm\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "# missed: mini-batch, N-fold cross validation\n",
    "# save the model: the structure of neural network, the weight matrix, the bias\n",
    "# run on the gpu\n",
    "# other rnn structure: bi-direction rnn\n",
    "####\n",
    "\n",
    "\n",
    "###########################  Data Access and Pre-process ########################### \n",
    "def read_data(filename):\n",
    "    # num_prob shall be the info from the dataset.\n",
    "    # num_steps_max shall not be specified.\n",
    "    records = []\n",
    "    num_steps_max = 0\n",
    "    num_probs = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        num_steps, seq_probs, seq_tags = None, None, None\n",
    "        for i, row in enumerate(f):\n",
    "            try:\n",
    "                row_0 = row\n",
    "                row = list(map(int, row.strip().split(\",\")))\n",
    "                if i % 4 == 1:\n",
    "                    num_steps = row[0]\n",
    "                elif i % 4 == 2:\n",
    "                    seq_probs = row\n",
    "                elif i % 4 == 3:\n",
    "                    seq_tags = row\n",
    "                    if (num_steps >= 3) and num_steps and seq_probs and seq_tags:\n",
    "                        num_steps_max = max(num_steps_max, num_steps)\n",
    "                        num_probs = max([num_probs] + seq_probs)\n",
    "                        records += [(num_steps, seq_probs, seq_tags)]\n",
    "            except:\n",
    "                if i % 4 == 1:\n",
    "                    num_steps = None\n",
    "                elif i % 4 == 2:\n",
    "                    seq_probs = None\n",
    "                elif i % 4 == 3:\n",
    "                    seq_tags = None                    \n",
    "                print(\"- broken line in {} : {}\".format(i, row_0))                \n",
    "    return records, num_steps_max, num_probs+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './0910_b_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9b816b6f5dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0910_b_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0910_b_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrecords_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps_max_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_probs_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrecords_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps_max_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_probs_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# num_steps = max(num_steps_max_train, num_steps_max_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5c92778111dd>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mnum_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './0910_b_train.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "train_file = os.path.join(DATA_DIR, './train.csv')\n",
    "test_file = os.path.join(DATA_DIR, './test.csv')\n",
    "records_train, num_steps_max_train, num_probs_train = read_data(train_file)\n",
    "records_test, num_steps_max_test, num_probs_test = read_data(test_file)\n",
    "# num_steps = max(num_steps_max_train, num_steps_max_test)\n",
    "num_probs = max(num_probs_train, num_probs_test)\n",
    "# num_steps_max_test: 1062\n",
    "# num_probs_test: 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(dtype=-1, cv=None, sample=False):\n",
    "    if sample == False:\n",
    "        if dtype == -1:\n",
    "            return list(range(len(records_train)))\n",
    "        if dtype == 0:\n",
    "            return list(range(len(records_test)))\n",
    "        if dtype == 1:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(idx, dtype=-1):\n",
    "    # one_hot for both x and y\n",
    "    if dtype == -1:  # train\n",
    "        return records_train[idx]\n",
    "    elif dtype == 0: # valid\n",
    "        return records_test[idx]\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(idxs, dtype=-1, cv=None, sample=False):\n",
    "    if dtype == 1:\n",
    "        return None, None\n",
    "    \n",
    "    s_seq = []\n",
    "    x_seq = []\n",
    "    y_seq = []\n",
    "    for idx in idxs:\n",
    "        s, x_inp, y_inp = preprocess(idx, dtype)\n",
    "        s_seq += [s]\n",
    "        x_seq += [x_inp]\n",
    "        y_seq += [y_inp]\n",
    "\n",
    "        \n",
    "\n",
    "#padding skill_sequences with -1, padding ans_sequences with 0\n",
    "\n",
    "\n",
    "\n",
    "    num_steps = max(s_seq)\n",
    "    for i in range(len(x_seq)):\n",
    "        x_seq[i] = x_seq[i][0:num_steps] + [-1] * (num_steps-len(x_seq[i]))  if len(x_seq[i]) < num_steps else x_seq[i][0:num_steps]\n",
    "        y_seq[i] = y_seq[i][0:num_steps] + [0] * (num_steps-len(y_seq[i])) if len(y_seq[i]) < num_steps else y_seq[i][0:num_steps]\n",
    "    \n",
    "    x_feed = {\"X_ph\": np.array(x_seq)}\n",
    "    y_feed = {\"Y_ph\": np.array(y_seq)}\n",
    "    \n",
    "    return x_feed, y_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################  START RNN ########################### \n",
    "# X_ph (seq_probs)   :\n",
    "# Y_ph (seq_tags)    :\n",
    "#                    : batch_size x num_steps\n",
    "\n",
    "def seq_onehot(seq_probs, seq_tags, num_steps, num_probs):\n",
    "    seq_probs_ = tf.one_hot(seq_probs, depth=num_probs)\n",
    "    seq_probs_flat = tf.reshape(seq_probs_, [-1, num_probs])\n",
    "    \n",
    "    # element-wise multiplication between Matrix and Vector\n",
    "    # the i-th column of Matrixelement-wisedly multiply the i-th element in the Vector\n",
    "    \n",
    "    seq_tags_ = tf.cast(tf.reshape(seq_tags, [-1]), dtype=tf.float32)\n",
    "    seq_tags_ = tf.multiply(tf.transpose(seq_probs_flat), seq_tags_)\n",
    "    seq_tags_ = tf.reshape(tf.transpose(seq_tags_), shape=[-1, num_steps, num_probs])\n",
    "    return seq_tags_ * 2 - seq_probs_, seq_tags_\n",
    "\n",
    "\n",
    "'''\n",
    "return :\n",
    "[batch_size, num_steps, num_probs], \n",
    "[s, a, b] = 1 => student s answer problem a correctly, \n",
    "[s, a, b] = 0 => ..... did not answer\n",
    "[s, a, b] = -1=> ...... incorrect\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "num_layers = 1\n",
    "state_size = 200\n",
    "\n",
    "X_ph = tf.placeholder(tf.int32, [None, None])\n",
    "Y_ph = tf.placeholder(tf.int32, [None, None])\n",
    "keep_prob_ph = tf.placeholder(tf.float32)\n",
    "\n",
    "num_steps = tf.shape(X_ph)[1]\n",
    "print(num_steps)\n",
    "X_in, Y_in = seq_onehot(X_ph, Y_ph, num_steps, num_probs)\n",
    "\n",
    "## build up the network\n",
    "cells = [tf.contrib.rnn.LSTMCell(num_units=state_size, forget_bias=1.0, state_is_tuple=True) for _ in range(num_layers)]\n",
    "cells = [tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob_ph) for cell in cells]\n",
    "\n",
    "rnn_outputs_in_list = []\n",
    "rnn_inputs = X_in\n",
    "for i, cell in enumerate(cells):\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, time_major=False, scope=\"rnn-layer-\"+str(i), dtype = tf.float32)\n",
    "    rnn_outputs_in_list += [rnn_outputs]\n",
    "    rnn_inputs = rnn_outputs\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_probs])\n",
    "    b = tf.get_variable('b', [num_probs], initializer = tf.constant_initializer(0.0))\n",
    "\n",
    "Y_out = tf.matmul(tf.reshape(tf.tanh(rnn_outputs), [-1, state_size]), W) + b\n",
    "Y_out = tf.sigmoid(tf.reshape(Y_out, [-1, num_steps, num_probs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################  Define Loss  ########################### \n",
    "# Y_out: batch_size x num_steps x num_probs\n",
    "# why split?\n",
    "_, X_in_next = tf.split(X_in, num_or_size_splits = [1, num_steps-1], axis=1)\n",
    "Y_out_cur, _ = tf.split(Y_out, num_or_size_splits = [num_steps-1, 1], axis=1)\n",
    "_, Y_in_next = tf.split(Y_in, num_or_size_splits = [1, num_steps-1], axis=1)\n",
    "\n",
    "# this code block calculate the loss using tf.gather_nd\n",
    "idx_selected = tf.where(tf.not_equal(X_in_next, 0))\n",
    "Y_out_selected = tf.gather_nd(Y_out_cur, idx_selected)\n",
    "Y_in_selected = tf.gather_nd(Y_in_next, idx_selected)\n",
    "\n",
    "loss = -Y_in_selected * tf.log(Y_out_selected) - (1-Y_in_selected) * tf.log(1-Y_out_selected)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################  Mini-batch  ########################### \n",
    "def shuffle(dtype=-1, cv=None, mode=-1, epoch=1):\n",
    "    data = np.array(parse(dtype, cv))\n",
    "    if data is None: return\n",
    "    \n",
    "    size = len(data)\n",
    "    \n",
    "    batch_per_epoch = int(size / batch_size) + bool(size % batch_size) * (mode != -1)\n",
    "    \n",
    "    total = epoch * batch_per_epoch\n",
    "    yield total # total batch number\n",
    "    \n",
    "    num_epoch = epoch if mode == -1 else 1\n",
    "    for i in range(num_epoch):        \n",
    "        idx_shuffle = perm(np.arange(size)) if mode == -1 else np.arange(size)\n",
    "        \n",
    "        for b in range(batch_per_epoch):\n",
    "            if (b+1) == batch_per_epoch and mode == -1:\n",
    "                idx_in_list = idx_shuffle[b*batch_size:]\n",
    "            else:\n",
    "                idx_in_list = idx_shuffle[(b*batch_size):(b+1)*batch_size]\n",
    "                \n",
    "            x_batch, y_batch = batch(data[idx_in_list], dtype, cv)\n",
    "            \n",
    "            yield (x_batch, y_batch, i, b, (b+1)==batch_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################  Evaluation function  ########################### \n",
    "def evaluate(sess, mode=-1):\n",
    "    \"\"\"\n",
    "    auc score\n",
    "    \"\"\"\n",
    "    def auc_score(prob_pred, prob_true):\n",
    "            fpr, tpr, thres = roc_curve(prob_true, prob_pred, pos_label=1)\n",
    "            return auc(fpr, tpr)\n",
    "\n",
    "    batches = shuffle(dtype=0, cv=None, mode=mode, epoch=1)\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i, packet in enumerate(batches):\n",
    "        if i == 0:\n",
    "            total = packet\n",
    "        else:\n",
    "            x_batch, y_batch, idx_epoch, idx_batch, end_batch = packet\n",
    "            y_out, y_in = sess.run((Y_out_selected, Y_in_selected),\n",
    "                                   feed_dict={ X_ph: x_batch[\"X_ph\"],\n",
    "                                               Y_ph: y_batch[\"Y_ph\"],\n",
    "                                               keep_prob_ph: 1.0,\n",
    "                                           }\n",
    "                               )\n",
    "            y_pred += [y_out]\n",
    "            y_true += [y_in]\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    return auc_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################  Training process  ########################### \n",
    "def optimize(sess, num_epochs):\n",
    "    batches = shuffle(dtype=-1, cv=None, mode=-1, epoch = num_epochs)\n",
    "    \n",
    "    for i, packet in enumerate(batches):\n",
    "        if i == 0:\n",
    "            total = packet\n",
    "            auc_train = evaluate(sess, mode=-1)\n",
    "            auc_test = evaluate(sess, mode=0)\n",
    "            print((\"[eval] Epoch {0:>4},  train auc {2:.5}, test auc: {2:.5}\".format(-1, auc_train, auc_test)))\n",
    "        else:\n",
    "            x_batch, y_batch, idx_epoch, idx_batch, end_batch = packet\n",
    "            sess.run(optimizer,\n",
    "                     feed_dict={ X_ph: x_batch[\"X_ph\"],\n",
    "                                 Y_ph: y_batch[\"Y_ph\"],\n",
    "                                 keep_prob_ph: num_probs,\n",
    "                             }\n",
    "                     )\n",
    "            \n",
    "            if idx_batch % 20 == 0:\n",
    "                total_loss_eval, = sess.run((total_loss, ),\n",
    "                                            feed_dict={  X_ph: x_batch[\"X_ph\"],\n",
    "                                                         Y_ph: y_batch[\"Y_ph\"],\n",
    "                                                         keep_prob_ph: 1.0,\n",
    "                                                    }\n",
    "                                        )\n",
    "                print((\"Epoch {0:>4}, iteration {1:>4}, batch loss value: {2:.5}\".format(idx_epoch, idx_batch, total_loss_eval)))\n",
    "                save_path = saver.save(sess, \"/saved_model/model.ckpt\")\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "            if end_batch:\n",
    "                auc_train = evaluate(sess, mode=-1)\n",
    "                auc_test = evaluate(sess, mode=0)\n",
    "                print((\"[eval] Epoch {0:>4}, train auc {2:.5}, test auc: {2:.5}\".format(idx_epoch, auc_train, auc_test)))\n",
    "                save_path = saver.save(sess, \"/saved_model/model.ckpt\")\n",
    "                print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraction_batch(data):\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraction(sess):\n",
    "    \n",
    "    #input a variable with form [student_id, number of steps, [#skill], [correct]]\n",
    "    #let the name be data_input first\n",
    "    #feed the variable for training to extract y_out\n",
    "    y_out = sess.run((Y_out),feed_dict={ X_ph: x_batch[\"X_ph\"],\n",
    "                                               Y_ph: y_batch[\"Y_ph\"],\n",
    "                                               keep_prob_ph: 1.0,})\n",
    "    \n",
    "    #Batch_state: [(ITEST_id1, state_vector1), (ITEST_id2, state_vector2), ....]\n",
    "    batch_state = [(data_input[i][0],y_out[i][data_input[i][1]-1]) for i in range(len(y_out))]\n",
    "    \n",
    "    #Collect ITEST_id into a dictionary with key [ITEST_id1, ITEST_id2, ITEST_id3, ...]\n",
    "    output = {\"ITEST_id\": [data_input[i][0] for i in range(len(datainput))]}\n",
    "    \n",
    "    #Collect Skills into output with the format {'SKill 1': [probability for student 1, probability for student 2, ...]}\n",
    "    ouput.update(dict([(skill_list[i], [row[1][i] for row in y_out]) for i in range(len(skill_list))]))\n",
    "    \n",
    "    #Put the whole thing into a pandas dataframe\n",
    "    output = pandas.DataFrame(data)\n",
    "    \n",
    "    #Set ITEST_id as the index of the output\n",
    "    output.set_index('ITEST_id')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e4f4a9eabf36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpermutation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "WITH_CONFIG = True\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "#Restore Model\n",
    "restore = 0  #0: not restore, 1: restore\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "if WITH_CONFIG:\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # specify the GPU to run\n",
    "    config.gpu_options.visible_device_list = '1'\n",
    "    with tf.Session(config=config) as sess:\n",
    "        if restore:\n",
    "            saver.restore(sess, \"/saved_model/model.ckpt\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        optimize(sess, num_epochs)\n",
    "else:\n",
    "    with tf.Session() as sess:\n",
    "        if restore:\n",
    "            saver.restore(sess, \"/saved_model/model.ckpt\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        optimize(sess, num_epochs)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print((\"program run for: {0}s\".format(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
